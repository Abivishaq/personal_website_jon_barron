<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Abivishaq Balasubramanian</title>

    <meta name="author" content="Abivishaq ">
    <!-- Site template originally by Jon Barron -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Abivishaq Balasubramanian
                </p>
                <p>I’m a Research Engineer at Georgia Tech with an M.S. in Robotics (Georgia Tech). My focus is robot learning, particularly for field robotics. I’ve worked across perception, control, navigation, localization, manipulation, and human–robot interaction, and on platforms including multirotors, quadrupeds, wheeled UGVs, and mobile manipulators (Stretch RE2), as well as VTOL and fixed-wing aircraft.
                </p>
                <p style="text-align:center">
                  <a href="mailto:abivishaq@gatech.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Abivishaq_Resume.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=71XmDvYAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://linkedin.com/in/abivishaq">linkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Abivishaq">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile_photo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile_photo.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research and Projects</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <!-- Fetch-to-Stretch Grasping: 6-DoF→4-DoF Pose Adaptation in ROS -->
            <tr>
              <td style="padding:20px;width:50%;vertical-align:middle">
                <div style="max-width:320px"><video style="width:100%;height:auto;display:block;border-radius:8px"
                          autoplay muted loop playsinline preload="metadata">
                      <source src="videos/Rerail Stretchit Grasp.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video></div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <div style="display:flex;align-items:baseline;gap:8px;flex-wrap:wrap"></div>
                  <span class="projectTitle">Fetch-to-Stretch Grasping: 6-DoF→4-DoF Pose Adaptation in ROS</span>
                      <a href="https://github.com/gt-rail-internal/rerail_stretchit_grasp" target="_blank" rel="noopener"
                        style="font-size:12px;padding:4px 8px;border:1px solid #ccc;border-radius:999px;text-decoration:none;line-height:1;display:inline-block">
                        Code ↗
                      </a>
                  
                </div>
                <br><br>
                <p style="margin:0">Developed the grasping module for the Stretch RE2 mobile manipulator by adapting a grasping algorithm originally designed for Fetch. My primary contribution was integrating the grasping logic with Stretch’s ROS ecosystem and modifying the grasp pose to accommodate its hardware constraints, converting 6-DoF grasp poses—suited to Fetch’s arm—into 4-DoF poses compatible with Stretch’s simpler kinematics.</p>
              </td>
            </tr>
            <!-- ------------------------------------------------------------------------ -->
            
            <!-- M2M-Depth: Monocular-to-Metric Depth -->
            <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><img src="images/depth_anything.png" alt="M2M-Depth: Monocular-to-Metric Depth thumbnail"
                          style="width:100%;height:auto;display:block;border-radius:8px"
                          loading="lazy"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="projectTitle">M2M-Depth: Monocular-to-Metric Depth</span>
                  <br><br>
                  <p style="margin:0">Depth Anything predicts relative (scale-ambiguous) depth from RGB. We learn a mapping that aligns these predictions to metric depth using partial metric depth measurements from an RGB-D sensor. The calibrated depth yields denser, more accurate point clouds.</p>
                </td>
              </tr>
              <!-- ---------------------------------------------------------------------------- -->
              
              <!-- Multirotor Pursuit Guidance -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><video style="width:100%;height:auto;display:block;border-radius:8px"
                            autoplay muted loop playsinline preload="metadata">
                        <source src="videos/Visual_drone_tracking_and_forecasting.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="projectTitle">Multirotor Pursuit Guidance</span>
                  <br><br>
                  <p style="margin:0">Developed a guidance system for a multirotor to pursue an adversarial drone. In field tests, achieved an 8-s capture from an initial 50-m separation against targets moving tangentially. In simulation, tested a vision-based tracking system and visualized predicted target trajectories.</p>
                </td>
              </tr>
              <!-- --------------------------------------------------------------------------- -->

              <!-- Quasdruped gait -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><video style="width:100%;height:auto;display:block;border-radius:8px"
                            autoplay muted loop playsinline preload="metadata">
                        <source src="videos/quadruped_walking.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="projectTitle">In-House Quadruped: GAIT implementation</span>
                  <br><br>
                  <p style="margin:0">Integrated Jetson–CAN motor control and a basic open-source gait on an in-house quadruped. Achieved~1.5 m/s speed on flat ground.</p>
                </td>
              </tr>
              <!-- ---------------------------------------------------------------------------------- -->
              
              <!-- Unet segmentation -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><img src="images/Unet_segmentation.png" alt="SkyMap: Drone-Derived Costmaps for Quadruped Guidance thumbnail"
                          style="width:100%;height:auto;display:block;border-radius:8px"
                          loading="lazy"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="projectTitle">SkyMap: Drone-Derived Costmaps for Quadruped Guidance</span>
                  <br><br>
                  <p style="margin:0">We built a heterogeneous drone–quadruped team. Using the overhead drone’s camera and GPS, we localized the quadruped from the air and generated a bird’s-eye-view costmap. This aerial costmap enabled the quadruped to navigate quickly in previously unseen environments without time-consuming local exploration.</p>
                </td>
              </tr>
              <!-- ----------------------------------------------------------------------------------- -->

              <!-- Suboptimal demos -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><video style="width:100%;height:auto;display:block;border-radius:8px"
                            autoplay muted loop playsinline preload="metadata">
                        <source src="videos/IRL_vid.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="data/IRL_Project_Final.pdf"><span class="projectTitle">Reward Regression from Distraction-Induced Suboptimal Demonstrations</span></a>
                  <br><br>
                  <p style="margin:0">Prior work simulates varying demonstration quality by injecting noise into suboptimal demonstrations and shows that regressing backward can learn a policy that outperforms the demonstrations. We instead induce genuine variation in human suboptimality by introducing controlled distractions during data collection, achieving higher reward correlation (R<sup>2</sup> = 0.997) than prior work (R<sup>2</sup> = 0.942).</p>
                </td>
              </tr>
              <!-- --------------------------------------------------------------------------------------- -->
              
              <!-- Transfer learning for manipulation -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><video style="width:100%;height:auto;display:block;border-radius:8px"
                            autoplay muted loop playsinline preload="metadata">
                        <source src="videos/pick_learned.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="data/manip_transfer_learning.pdf"><span class="projectTitle">When Does Transfer Help? Data-Dependent Transfer in Robotic Manipulation</span></a>
                  <br><br>
                  <p style="margin:0">We study transfer learning in robotic manipulation by transferring skills between cube pick and push tasks, comparing PPO and Diffusion Policy across varying demonstration sizes. Transfer is effective with sufficient data—especially with Diffusion Policy—yielding modest speedups; with small datasets the benefits diminish, variance increases, and success rates drop.</p>
                </td>
              </tr>
              <!-- --------------------------------------------------------------------------------------------- -->
              
              <!-- Open vocab reflect -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><video style="width:100%;height:auto;display:block;border-radius:8px"
                            autoplay muted loop playsinline preload="metadata">
                        <source src="videos/make_cofee_8x.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="data/open_vocab_robot_failure_detection_and_correction.pdf"><span class="projectTitle">Foundation Models for Enhanced Understanding of Open-World Environments in Failure Recovery</span></a>
                  <br><br>
                  <p style="margin:0">We build a failure-recovery framework that explains robot mistakes in natural language by structuring perception as an open-vocabulary scene graph. Using a VLM to extract objects, states, and relations, the system detects, localizes, and explains failures—then can draft corrective plans with an LLM.</p>
                </td>
              </tr>
              <!-- ------------------------------------------------------------------------------------------------ -->

              <!-- VRU -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><img src="images/VRU.png" alt="Foveated Multi-Resolution VRU Detection thumbnail"
                          style="width:100%;height:auto;display:block;border-radius:8px"
                          loading="lazy"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <span class="projectTitle">Foveated Multi-Resolution VRU Detection</span>
                  <br><br>
                  <p style="margin:0">We exploited structure in the data to achieve a 2× inference speedup while maintaining detection performance. In self-driving/ADAS imagery, far-off objects of interest are small and tend to cluster near the horizon (i.e., around the image’s central band). We therefore run detection at full resolution in this region and process the rest of the image at a lower resolution, yielding faster end-to-end inference without degrading accuracy.</p>
                </td>
              </tr>
              <!-- ------------------------------------------------------------------------------------------------------- -->

              <!-- XAI for PA -->
              <tr>
                <td style="padding:20px;width:50%;vertical-align:middle">
                  <div style="max-width:320px"><img src="images/XAI_for_PA.png" alt="Explaining Proactivity: Taxonomy and Counterfactuals for Robot Actions thumbnail"
                          style="width:100%;height:auto;display:block;border-radius:8px"
                          loading="lazy"></div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <div style="display:flex;align-items:baseline;gap:8px;flex-wrap:wrap"></div>
                  <span class="projectTitle">Explaining Proactivity: Taxonomy and Counterfactuals for Robot Actions</span>
                    <a href="https://github.com/Abivishaq/ExplainableSpatioTemporalObjectTracking" target="_blank" rel="noopener"
                        style="display:inline-block;font-size:12px;padding:4px 8px;border:1px solid #ccc;border-radius:999px;text-decoration:none">
                        Code ↗
                      </a>
                  </div>
                  <br><br>
                  <p style="margin:0">We present a taxonomy of proactive robot behaviors and show how different sources of proactivity translate into concrete actions. Building on this, we develop a counterfactual-based explainer for prior proactive methods. We will evaluate its impact in a user study measuring how the explanations affect understanding and trust.</p>
                </td>
              </tr>
              <!-- ------------------------------------------------------------------------------------------------------- -->
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
